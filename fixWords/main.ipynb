{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d796cef23297fd7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T15:04:14.363498Z",
     "start_time": "2024-03-26T15:03:34.764735Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from heapq import heappush, heappop\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"MonoHime/ru_sentiment_dataset\")\n",
    "sentences = dataset['train']['text']\n",
    "text = \"\"\n",
    "for i in range(10000, len(sentences) // 10):\n",
    "    text += sentences[i] "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' абвгдежзийклмнопрстуфхцчшщъыьэюя'\n",
      "0 -15248.792990777416\n",
      "1 -12872.423467606657\n",
      "2 -10734.061519962917\n",
      "3 -8668.154229419846\n",
      "4 -7166.513216102878\n"
     ]
    }
   ],
   "source": [
    "class LanguageNgramModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, за какими буквами следуют какие.\n",
    "    Параметры конструктора:\n",
    "    order - порядок (сколько предыдущих букв помнит модель), или n-1\n",
    "    smoothing - величина, добавляемая к каждому счётчику букв для устойчивости\n",
    "    recursive - вес, с которым используется модель на один порядок меньше\n",
    "    Обучаемые параметры:\n",
    "    counter_ - хранилище частот n-грам, в виде словаря счётчиков.\n",
    "    vocabulary_ - множество всех символов, учитываемых моделью\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Оценка числа всех буквосочетаний по тексту\n",
    "        Параметры:\n",
    "        corpus - текстовая строка.\n",
    "        \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "\n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Оценка частоты всех символов, которые могут следовать за контекстом\n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает:\n",
    "        freq - вектор условных частот букв, в форме pandas.Series\n",
    "        \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "\n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Сглаженная оценка вероятности всех символов, которые могут следовать за контекстом\n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает:\n",
    "        freq - вектор условных вероятностей букв, в форме pandas.Series  \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "\n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Оценка логарифма вероятности конкретного продолжения данной фразы.\n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "\n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Оценка вероятности конкретного продолжения данной фразы.\n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))\n",
    "\n",
    "\n",
    "class MissingLetterModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, какие буквы обычно исключаются из сокращений\n",
    "    Параметры:\n",
    "    order - порядок, или n+1\n",
    "    smoothing_missed - число, прибавляемое к счётчику пропущенных символов\n",
    "    smoothing_total - число, прибавляемое к счётчику всех символов\n",
    "    \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "\n",
    "    def fit(self, sentence_pairs):\n",
    "        \"\"\" Оценка частоты сокращения символов на основе обучающих примеров\n",
    "        Параметры:\n",
    "        sentence_pairs - список пар (исходная фраза, сокращение)\n",
    "        В сокращении все пропущенные символы заменены на дефисы.\n",
    "        \"\"\"\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1\n",
    "\n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Оценка вероятности того, что символ last_letter пропущен после символов context\"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "\n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка логарифма вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется.\n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token != '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "\n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется.\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    \"\"\" Генерация вариантов расшифровки аббревиатуры (вспомогательная функция)\n",
    "    Параметры:\n",
    "    prefix_proba - правдоподобие расшифрованной части аббревиатуры\n",
    "    prefix - расшифрованная часть аббревиатуры\n",
    "    suffix - не расшифрованная часть аббревиатуры\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    cache - хранилище оценок качества концов слова\n",
    "    Возвращает: список опций в форме (оценка качества, расшифрованная часть,\n",
    "        не расшифрованная часть, новая буква, оценка качества не расшифрованной части)\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # тут мы считаем, что буква была пропущена\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # тут мы считаем, что пропущенной буквы не было\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# считываем текст\n",
    "# with open('Pofigizm_book.txt', encoding = 'utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# оставляем только буквы и пробелы в тексте\n",
    "text2 = re.sub(r'[^а-я ]+', '', text.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters)) # ' абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "# готовим обучающую выборку для модели опечаток:\n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # тут считаем все буквы пропущенными\n",
    "    + [(all_letters, all_letters)] * 10 # тут считаем все буквы НЕ пропущенными\n",
    "    + [('аоуыэеёиюя', '----------')] * 30 # тут считаем пропущенными только гласные\n",
    ")\n",
    "# обучаем обе модели\n",
    "big_lang_m = LanguageNgramModel(order=6, smoothing=0.005, recursive=0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)\n",
    "\n",
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 0.001, 0.01)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T15:07:08.178444Z",
     "start_time": "2024-03-26T15:04:14.388491Z"
    }
   },
   "id": "a7331b8cca5d95e4",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca7a24c42f07a440",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T15:18:38.706166Z",
     "start_time": "2024-03-26T15:15:23.840151Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 65\u001B[0m\n\u001B[0;32m     62\u001B[0m             result[candidate[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m candidate[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m---> 65\u001B[0m \u001B[43mnoisy_channel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mспасбо спс пжлст до встрч\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbig_lang_m\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbig_err_m\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[25], line 52\u001B[0m, in \u001B[0;36mnoisy_channel\u001B[1;34m(word, lang_model, missed_model, freedom, max_attempts, optimism, verbose)\u001B[0m\n\u001B[0;32m     50\u001B[0m prefix \u001B[38;5;241m=\u001B[39m next_best[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     51\u001B[0m suffix \u001B[38;5;241m=\u001B[39m next_best[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m---> 52\u001B[0m new_options \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_options\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprefix_proba\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissed_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimism\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# add only the solution potentioally no worse than the best + freedom\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m new_option \u001B[38;5;129;01min\u001B[39;00m new_options:\n",
      "Cell \u001B[1;32mIn[19], line 169\u001B[0m, in \u001B[0;36mgenerate_options\u001B[1;34m(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\u001B[0m\n\u001B[0;32m    167\u001B[0m     new_prefix \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m next_letter\n\u001B[0;32m    168\u001B[0m     proba_missing_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mlog((\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m missed_model\u001B[38;5;241m.\u001B[39mpredict_proba(prefix, next_letter)))\n\u001B[1;32m--> 169\u001B[0m proba_next_letter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(\u001B[43mlang_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msingle_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_letter\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache:\n\u001B[0;32m    171\u001B[0m     proba_suffix \u001B[38;5;241m=\u001B[39m cache[\u001B[38;5;28mlen\u001B[39m(new_suffix)] \u001B[38;5;241m*\u001B[39m optimism\n",
      "Cell \u001B[1;32mIn[19], line 80\u001B[0m, in \u001B[0;36mLanguageNgramModel.single_proba\u001B[1;34m(self, context, continuation)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msingle_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, context, continuation):\n\u001B[0;32m     75\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Оценка вероятности конкретного продолжения данной фразы.\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;124;03m    Параметры:\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;124;03m    context - текстовая строка, известное начало фразы\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;124;03m    continuation - текстовая строка, гипотетическое продолжение фразы\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msingle_log_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontinuation\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[19], line 70\u001B[0m, in \u001B[0;36mLanguageNgramModel.single_log_proba\u001B[1;34m(self, context, continuation)\u001B[0m\n\u001B[0;32m     68\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m continuation:\n\u001B[1;32m---> 70\u001B[0m     result \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m[token])\n\u001B[0;32m     71\u001B[0m     context \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m token\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "Cell \u001B[1;32mIn[19], line 59\u001B[0m, in \u001B[0;36mLanguageNgramModel.predict_proba\u001B[1;34m(self, context)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, context):\n\u001B[0;32m     54\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Сглаженная оценка вероятности всех символов, которые могут следовать за контекстом\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03m    Параметры:\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    context - текстовая строка (учиываются только последние self.order символов)\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03m    Возвращает:\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m    freq - вектор условных вероятностей букв, в форме pandas.Series  \"\"\"\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     counts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m counts \u001B[38;5;241m/\u001B[39m counts\u001B[38;5;241m.\u001B[39msum()\n",
      "Cell \u001B[1;32mIn[19], line 49\u001B[0m, in \u001B[0;36mLanguageNgramModel.get_counts\u001B[1;34m(self, context)\u001B[0m\n\u001B[0;32m     47\u001B[0m     freq[token] \u001B[38;5;241m=\u001B[39m freq_dict[token] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmoothing\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecursive \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morder \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 49\u001B[0m     child_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchild_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecursive\n\u001B[0;32m     50\u001B[0m     freq \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m child_freq\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m freq\n",
      "Cell \u001B[1;32mIn[19], line 49\u001B[0m, in \u001B[0;36mLanguageNgramModel.get_counts\u001B[1;34m(self, context)\u001B[0m\n\u001B[0;32m     47\u001B[0m     freq[token] \u001B[38;5;241m=\u001B[39m freq_dict[token] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmoothing\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecursive \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morder \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 49\u001B[0m     child_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchild_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecursive\u001B[49m\n\u001B[0;32m     50\u001B[0m     freq \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m child_freq\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m freq\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN_2024\\venv\\Lib\\site-packages\\pandas\\core\\ops\\common.py:62\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m     59\u001B[0m stripped_name \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39mremoveprefix(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mremovesuffix(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     60\u001B[0m is_cmp \u001B[38;5;241m=\u001B[39m stripped_name \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meq\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mne\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mle\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mge\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(method)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_method\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_cmp \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCIndex) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, ABCSeries):\n\u001B[0;32m     65\u001B[0m         \u001B[38;5;66;03m# For comparison ops, Index does *not* defer to Series\u001B[39;00m\n\u001B[0;32m     66\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=3.0, max_attempts=2000, optimism=0.92, verbose=False):\n",
    "    \"\"\" Подбор фраз, аббревиатурой которых может быть word\n",
    "    Параметры:\n",
    "    word - аббревиатура\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    freedom - возможный зазор по оценке логарифма правдоподобия кандидатов\n",
    "    max_attempts - число итераций\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    verbose - печатать ли наилучших текущих кандидатов в ходе исполнения функции\n",
    "    Возвращает: словарик с ключами - расшифровками\n",
    "        и значениями - минус логарифмом правдоподобия расшифровок.\n",
    "        Чем меньше значение, тем правдоподобнее расшифровка.\n",
    "    \"\"\"\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # добавляем в кучу пустое начало\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # добавляем в кандидаты расшифровку по умолчанию - без пропущенных букв\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        print('baseline score is', best_logprob)\n",
    "    # готовим хранилище вероятностей конфов слов\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "\n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # слово расшифровано до конца\n",
    "            # если оно достаточно хорошее, добавим его в кандидаты\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # обновим наилучшую оценку правдоподобия\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options:\n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result\n",
    "\n",
    "noisy_channel('спасбо спс пжлст до встрч', big_lang_m, big_err_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
